{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d92e7-10aa-44db-8957-e25901a96792",
   "metadata": {},
   "source": [
    "## Stable Diffusion on PVC with IPEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fdf55-0df7-4964-ac04-16dc31cfe5dd",
   "metadata": {},
   "source": [
    "This is a demo of [Stable Diffusion with the Hugging Face API](https://huggingface.co/stabilityai), and using the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) to optimize the model pipeline on Intel® Data Center GPU Max Series.\n",
    "\n",
    "**Intro to Hardware: Intel® Data Center GPU Max Series**\n",
    "\n",
    "- Up to 408MB of L2 Cache\n",
    "- Built-in Ray Tracing Acceleration\n",
    "- Intel® Xe Matrix Extensions (XMX)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! clinfo -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff3ca2f",
   "metadata": {},
   "source": [
    "The demo consists of the following steps:\n",
    "\n",
    "1. Load and define the core SD model components from HF.\n",
    "2. Set up and run a standard SD pipeline with the HF API, i.e., generate a FP32 precision image.\n",
    "3. Optimize SD with IPEX-XPU, run the SD on Intel® Data Center GPU Max 1100.\n",
    "4. Compare the results wrt inference latency time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a85e09-aa28-4d3f-9357-e8f8e58e88dc",
   "metadata": {},
   "source": [
    "#### Select the proper kernel  \n",
    "\n",
    "Choose ipex_xpu kernel to run this demo.\n",
    "\n",
    "\n",
    "If you can't find the ipex_xpu kernel, please go back to terminal and type ```source prepare_env.sh``` to prepare the environments for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ebf84a4-42f9-4c8d-ae63-f209cac19a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, DPMSolverMultistepScheduler, EulerDiscreteScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# Define model ID for SD version\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c64c36-bae5-4877-8020-b0c349fbc998",
   "metadata": {},
   "source": [
    "Next, we construct the SD pipeline with the HF API. For different performances, experiment, e.g., with the scheduler, and its parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d8984-0383-4479-93f9-406ed48b6c80",
   "metadata": {},
   "source": [
    "**Single image inference**\n",
    "\n",
    "Then, we call the pipeline with a written description of the wanted image, i.e., the text prompt. And generate an image.\n",
    "\n",
    "The inference process can be made deterministic by setting the generator seed. Furthermore, through the number of inference steps, we can govern the quality of the image, i.e., more steps equals better quality. Reduce the number of steps to receive results faster.\n",
    "\n",
    "Please experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59b1f7ca-facb-44a6-8761-c206c4c5dd12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:22<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating one FP32 image took 24.17s\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for the image generation\n",
    "prompt = \"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"\n",
    "\n",
    "# Set the number of iterations for the image generation\n",
    "n_inf_steps = 20\n",
    "\n",
    "# # Setting seed for deterministic output\n",
    "# seed = 701\n",
    "# generator = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "# image = pipeline(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "image = pipe(prompt, num_inference_steps=n_inf_steps).images[0]\n",
    "end = time.time()\n",
    "sd_fp32_t = end-start\n",
    "print(f\"Generating one FP32 image took {round(sd_fp32_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_FP32.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28cb75d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:447: UserWarning: For XPU device, the split master weight is unsupported for now, so temp to disable it\n",
      "  warnings.warn(\"For XPU device, the split master weight is unsupported for now, so temp to disable it\")\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:457: UserWarning: For XPU device to save valuable device memory, temp to do optimization on inplaced model, so                     make inplace to be true\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:464: UserWarning: For XPU, the weight prepack and sample input are disabled. The onednn layout                     is automatically chosen to use\n",
      "  warnings.warn(\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:486: UserWarning: Conv BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Conv BatchNorm folding failed during the optimize process.\")\n",
      "/home/gta/miniconda3/envs/stable_d/lib/python3.9/site-packages/intel_extension_for_pytorch/frontend.py:491: UserWarning: Linear BatchNorm folding failed during the optimize process.\n",
      "  warnings.warn(\"Linear BatchNorm folding failed during the optimize process.\")\n"
     ]
    }
   ],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "\n",
    "pipe = pipe.to('xpu')\n",
    "\n",
    "# Put model in eval mode.\n",
    "pipe.unet.eval()\n",
    "\n",
    "# unet to device=xpu\n",
    "pipe.unet = pipe.unet.to('xpu')\n",
    "\n",
    "# Optimize the model w/ IPEX\n",
    "pipe.unet = ipex.optimize(pipe.unet)\n",
    "\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "with torch.cpu.amp.autocast():\n",
    "    image = pipe(prompt, num_inference_steps=n_inf_steps).images[0]\n",
    "end = time.time()\n",
    "sd_xpu_t = end-start\n",
    "print(f\"Generating one image on xpu took {round(sd_xpu_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_xpu.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f35c69",
   "metadata": {},
   "source": [
    "# Simple timing of inference on xpu\n",
    "start = time.time()\n",
    "with torch.xpu.amp.autocast():\n",
    "    image = pipe(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "end = time.time()\n",
    "sd_bf16_t = end-start\n",
    "print(f\"Generating one BF16 image took {round(sd_bf16_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_BF16.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
