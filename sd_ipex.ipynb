{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d92e7-10aa-44db-8957-e25901a96792",
   "metadata": {},
   "source": [
    "## Stable Diffusion on SPR with IPEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fdf55-0df7-4964-ac04-16dc31cfe5dd",
   "metadata": {},
   "source": [
    "This is a demo of [Stable Diffusion with the Hugging Face API](https://huggingface.co/stabilityai), and using the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) to optimize the model pipeline on Intel's 4th generation Xeon platform.\n",
    "\n",
    "The demo consists of the following steps:\n",
    "\n",
    "1. Load and define the core SD model components from HF.\n",
    "2. Set up and run a standard SD pipeline with the HF API, i.e., generate a FP32 precision image.\n",
    "3. Optimize SD with IPEX, using Auto Mixed Precision (BF16), and run the pipeline again.\n",
    "4. Compare the results wrt inference latency time.\n",
    "5. Run batched inference with the optimized SD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a85e09-aa28-4d3f-9357-e8f8e58e88dc",
   "metadata": {},
   "source": [
    "**This demo is executed in a Conda\\* environment.**\n",
    "\n",
    "The environment is the latest [Intel® oneAPI AI Analytics Toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html) PyTorch* environment, which includes Intel® Optimizations for deep learning workflows. See [here](https://software.intel.com/content/www/us/en/develop/articles/installing-ai-kit-with-conda.html) for more installation information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d10c3-6463-444d-9e03-e1f697987761",
   "metadata": {},
   "source": [
    "If not yet installed, please make sure to uncomment the following line in order to install Diffusers, Transformers, and update Torch and IPEX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b830f3d8-8fee-4bca-97a2-92247b00a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -U tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a624547-859c-40b1-b224-9fa61435e37b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b7f741b-bc0e-4d5b-a038-a8118257890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8b4a8e29184c4f9e9840a509f56585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, DPMSolverMultistepScheduler, EulerDiscreteScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# Define model ID for SD version\n",
    "model_id = \"stable-diffusion-2-1-base\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf84a4-42f9-4c8d-ae63-f209cac19a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e4d8984-0383-4479-93f9-406ed48b6c80",
   "metadata": {},
   "source": [
    "**Single image inference**\n",
    "\n",
    "Then, we call the pipeline with a written description of the wanted image, i.e., the text prompt. And generate an image.\n",
    "\n",
    "The inference process can be made deterministic by setting the generator seed. Furthermore, through the number of inference steps, we can govern the quality of the image, i.e., more steps equals better quality. Reduce the number of steps to receive results faster.\n",
    "\n",
    "Please experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59b1f7ca-facb-44a6-8761-c206c4c5dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fc7e26e7947e7b1949345d6541e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating one FP32 image took 8.9s\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for the image generation\n",
    "prompt = \"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"\n",
    "\n",
    "# Set the number of iterations for the image generation\n",
    "n_inf_steps = 10\n",
    "\n",
    "# Setting seed for deterministic output\n",
    "seed = 701\n",
    "generator = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "image = pipe(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "end = time.time()\n",
    "sd_fp32_t = end-start\n",
    "print(f\"Generating one FP32 image took {round(sd_fp32_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_FP32.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eeb3e0-e692-43d5-9a81-a8feccc439c3",
   "metadata": {},
   "source": [
    "**Optimization with IPEX**\n",
    "\n",
    "The UNET component of the model architecture is the one that uses most computational resources in the inference process. Hence, with IPEX, we optimize it, and put it in BF16 precision.\n",
    "Please note: Experiment also with the optimization of the text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a11ec4-f683-4668-87d4-19db9ba0b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "infer_dtype = torch.bfloat16\n",
    "\n",
    "# Put model in eval mode.\n",
    "pipe.unet.eval()\n",
    "# text_encoder.eval()\n",
    "\n",
    "# Optimize the model w/ IPEX\n",
    "pipe.unet = ipex.optimize(pipe.unet.eval(), dtype=infer_dtype, inplace=True)\n",
    "pipe.vae = ipex.optimize(pipe.vae.eval(), dtype=infer_dtype, inplace=True)\n",
    "pipe.text_encoder = ipex.optimize(pipe.text_encoder.eval(), dtype=infer_dtype, inplace=True)\n",
    "\n",
    "# Optimize with torch.compile\n",
    "pipe.unet = torch.compile(pipe.unet)\n",
    "pipe.vae = torch.compile(pipe.vae)\n",
    "pipe.text_encoder = torch.compile(pipe.text_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdb9cc8-6bee-4ad9-af56-255bb55eb2b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520b3e-2260-4182-bf1f-f13e72953696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-13 16:56:01,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-10-13 16:56:02,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-10-13 16:56:02,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-10-13 16:56:10,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
      "[2023-10-13 16:56:11,032] torch._inductor.graph: [INFO] Creating implicit fallback for:\n",
      "  target: torch_ipex.ipex_linear.default\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    ComputedBuffer(name='buf3', layout=FlexibleLayout('cpu', torch.bfloat16, size=[1, 77, 1024], stride=[78848, 1024, 1]), data=Pointwise(\n",
      "      'cpu',\n",
      "      torch.bfloat16,\n",
      "      tmp0 = load(arg373_1, i1)\n",
      "      tmp1 = load(arg0_1, i2 + 1024 * (tmp0))\n",
      "      tmp2 = load(arg372_1, i1)\n",
      "      tmp3 = load(arg1_1, i2 + 1024 * (tmp2))\n",
      "      tmp4 = tmp1 + tmp3\n",
      "      tmp5 = to_dtype(tmp4, torch.float32)\n",
      "      tmp6 = load(buf1, i1)\n",
      "      tmp7 = tmp5 - tmp6\n",
      "      tmp8 = load(buf2, i1)\n",
      "      tmp9 = index_expr(1024, torch.float32)\n",
      "      tmp10 = tmp8 / tmp9\n",
      "      tmp11 = constant(1e-05, torch.float32)\n",
      "      tmp12 = tmp10 + tmp11\n",
      "      tmp13 = rsqrt(tmp12)\n",
      "      tmp14 = tmp7 * tmp13\n",
      "      tmp15 = load(arg2_1, i2)\n",
      "      tmp16 = tmp14 * tmp15\n",
      "      tmp17 = load(arg3_1, i2)\n",
      "      tmp18 = tmp16 + tmp17\n",
      "      tmp19 = to_dtype(tmp18, torch.bfloat16)\n",
      "      return tmp19\n",
      "      ,\n",
      "      ranges=[1, 77, 1024],\n",
      "      origins={mul, convert_element_type_2, var_mean, arg373_1, sub, arg2_1, view, convert_element_type_1, add_3, arg1_1, add, arg3_1, add_2, embedding, slice_1, mul_1, embedding_1, arg372_1, rsqrt, arg0_1}\n",
      "    ))\n",
      "  ))\n",
      "  args[1]: TensorBox(StorageBox(\n",
      "    InputBuffer(name='arg5_1', layout=FixedLayout('cpu', torch.bfloat16, size=[1024, 1024], stride=[1024, 1]))\n",
      "  ))\n",
      "  args[2]: TensorBox(StorageBox(\n",
      "    InputBuffer(name='arg4_1', layout=FixedLayout('cpu', torch.bfloat16, size=[1024], stride=[1]))\n",
      "  ))\n",
      "  args[3]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      tmp0 = index_expr(0, torch.int32)\n",
      "      tmp1 = index_expr(0, torch.int32)\n",
      "      tmp2 = tmp0 == tmp1\n",
      "      tmp3 = constant(94493912075648, torch.int64)\n",
      "      tmp4 = load(buf4, 0)\n",
      "      tmp5 = where(tmp2, tmp3, tmp4)\n",
      "      return tmp5\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origins={full_like, empty, select_scatter, select}\n",
      "    )\n",
      "  ))\n",
      "  args[4]: 1024\n",
      "[2023-10-13 16:56:11,039] torch._inductor.graph: [INFO] Using FallbackKernel: torch.ops.torch_ipex.ipex_linear.default\n",
      "[2023-10-13 16:56:30,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
      "[2023-10-13 16:56:30,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052b9ff29d9c4878ba43e744d98efe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-13 16:56:30,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-10-13 16:56:33,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-10-13 16:56:33,956] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "with torch.cpu.amp.autocast():\n",
    "    image = pipe(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "end = time.time()\n",
    "sd_bf16_t = end-start\n",
    "print(f\"Generating one BF16 image took {round(sd_bf16_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_BF16.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66231297-c716-46ea-8214-6f670513ed0b",
   "metadata": {},
   "source": [
    "We compare the inference speeds between the two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41ce36-c71a-4bb0-9c0b-338957fe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference with IPEX, using AMP+BF16, was {round(sd_fp32_t/sd_bf16_t, 2)}x faster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694f007-b3f0-4e50-b1d0-e37cb65ceb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(outputdict):\n",
    "    fig = plt.figure(figsize = (10, 5)) \n",
    "    plt.bar(outputdict.keys(),outputdict.values(),color=['#ffd21e', '#0071c5'],width=0.4)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Eval inference (seconds); lower is better\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47de3d4-d205-45ac-8856-5b7ba6aba757",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDict={\"Full-precision\":sd_fp32_t,\"AMP (BF16)\":sd_bf16_t}\n",
    "plotter(outputDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7f3cd-7c82-4e67-bc76-45fb122b7509",
   "metadata": {},
   "source": [
    "**Batched inference**\n",
    "\n",
    "Finally, we generate a batch of 3 images, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de81bf81-a43c-486b-b8d6-c611fdd1c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9aae9e2-7ba3-40b1-ae75-a83de45fa897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed693d60e4042b38c5e46102d826af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 3 BF16 images took 60.24s. Per image inference time: 20.08s.\n"
     ]
    }
   ],
   "source": [
    "num_images = 3\n",
    "\n",
    "prompt = [\"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"] * num_images\n",
    "\n",
    "start = time.time()\n",
    "with torch.cpu.amp.autocast():\n",
    "    images = pipe(prompt, num_inference_steps=n_inf_steps).images\n",
    "end = time.time()\n",
    "sd_bbf16_t = end-start\n",
    "print(f\"Generating {num_images} BF16 images took {round(sd_bbf16_t, 2)}s. Per image inference time: {round(sd_bbf16_t/num_images, 2)}s.\")\n",
    "\n",
    "grid = image_grid(images, rows=1, cols=3)\n",
    "\n",
    "grid.save(f\"frog_batch.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce7a52e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipex_cpu",
   "language": "python",
   "name": "ipex_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
