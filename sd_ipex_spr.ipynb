{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6d92e7-10aa-44db-8957-e25901a96792",
   "metadata": {},
   "source": [
    "## Stable Diffusion on SPR with IPEX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1539f9",
   "metadata": {},
   "source": [
    "This is a demo of [Stable Diffusion with the Hugging Face API](https://huggingface.co/stabilityai), and using the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) to optimize the model pipeline on Intel's 4th generation Xeon platform.\n",
    "\n",
    "**Intro to Hardware: 4th Gen Intel速 Xeon速 Scalable Processor (Sapphire Rapids) - CPU specs**\n",
    "- Released in January of 2023\n",
    "- 2 sockets\n",
    "- 56 physical cores (112 vCPUs) per socket\n",
    "- 224 vCPUs total\n",
    "- New Intel速 Advanced Matrix Extensions (Intel速 AMX), a built-in AI acceleration engine.\n",
    "    - Supports BF16 and INT8 data types\n",
    "\n",
    "-----\n",
    "**PyTorch for CPU** \n",
    "- Intel's main goal is to make default PyTorch simple to use and performant on CPUs, by upstreaming all optimizations into the main branch of PyTorch. \n",
    "- Uses OpenMP for multi-threading\n",
    "- Upstreamed oneDNN provides optimized DNN primitives for Intel CPUs\n",
    "\n",
    "**Intel Extension for PyTorch (IPEX)**\n",
    "-  Provides additional acceleration that has not yet made it into the stock package\n",
    "- Early access to optimized kernels\n",
    "- Runtime extensions\n",
    "- Mixed precision with BF16 (at the time of writing - should soon be upstreamed into stock). \n",
    "    - BF16 data-type requires only half storage of FP32 data-type, which reduces both memory bandwidth and computation when applicable \n",
    "- Quantized inference\n",
    "- Async tasks\n",
    "- [IPEX GitHub link](https://github.com/intel/intel-extension-for-pytorch) | [IPEX Product page](https://www.intel.com/content/www/us/en/developer/tools/oneapi/optimization-for-pytorch.html#gs.qag6mc)\n",
    "\n",
    "For more details on oneDNN and BF16, you can take a look at this Medium article, published on 02/03/2021: [Accelerate PyTorch with IPEX and oneDNN using Intel BF16 Technology](https://medium.com/pytorch/accelerate-pytorch-with-ipex-and-onednn-using-intel-bf16-technology-dca5b8e6b58f).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb185d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "! lscpu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd47a50",
   "metadata": {},
   "source": [
    "#### Flags for AMX\n",
    "Under \"flags\", you can see `amx_bf16`, `amx_tile`, and `amx_int8`, so you know AMX is available for use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3fdf55-0df7-4964-ac04-16dc31cfe5dd",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The demo consists of the following steps:\n",
    "\n",
    "1. Load and define SD model from HF.\n",
    "2. Set up and run a standard SD pipeline with the HF API, i.e., generate a FP32 precision image.\n",
    "3. Optimize SD with IPEX, using Auto Mixed Precision (BF16), and run the pipeline again.\n",
    "4. Compare the results wrt inference latency time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7430ceb4",
   "metadata": {},
   "source": [
    "#### Environment Setup\n",
    "\n",
    "Ensure that ipex_cpu kernel is activated before running this demo.\n",
    "\n",
    "\n",
    "If you can't find the ipex_cpu kernel, please go back to terminal and type ```source prepare_env.sh``` to prepare the environments for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7f741b-bc0e-4d5b-a038-a8118257890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8b4a8e29184c4f9e9840a509f56585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, DPMSolverMultistepScheduler, EulerDiscreteScheduler, StableDiffusionPipeline, UNet2DConditionModel\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker\n",
    "\n",
    "from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "# Define model ID for SD version\n",
    "model_id = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d8984-0383-4479-93f9-406ed48b6c80",
   "metadata": {},
   "source": [
    "#### Single image inference\n",
    "\n",
    "Then, we call the pipeline with a written description of the wanted image, i.e., the text prompt. And generate an image.\n",
    "\n",
    "The inference process can be made deterministic by setting the generator seed. Furthermore, through the number of inference steps, we can govern the quality of the image, i.e., more steps equals better quality. Reduce the number of steps to receive results faster.\n",
    "\n",
    "Please experiment with your own prompts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1f7ca-facb-44a6-8761-c206c4c5dd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7fc7e26e7947e7b1949345d6541e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating one FP32 image took 8.9s\n"
     ]
    }
   ],
   "source": [
    "# Define the prompt for the image generation\n",
    "prompt = \"Painting of a frog with hat on a bicycle cycling in New York City at a beautiful dusk with a traffic jam and moody people in the style of Picasso\"\n",
    "\n",
    "# Set the number of iterations for the image generation\n",
    "n_inf_steps = 10\n",
    "\n",
    "# Setting seed for deterministic output\n",
    "seed = 701\n",
    "generator = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "image = pipe(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "end = time.time()\n",
    "sd_fp32_t = end-start\n",
    "print(f\"Generating one FP32 image took {round(sd_fp32_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_FP32.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eeb3e0-e692-43d5-9a81-a8feccc439c3",
   "metadata": {},
   "source": [
    "**Optimization with IPEX**\n",
    "\n",
    "The UNET component of the model architecture is the one that uses most computational resources in the inference process. Hence, with IPEX, we optimize it, and put it in BF16 precision.\n",
    "Please note: Experiment also with the optimization of the text encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a11ec4-f683-4668-87d4-19db9ba0b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "infer_dtype = torch.bfloat16\n",
    "\n",
    "# Put model in eval mode.\n",
    "pipe.unet.eval()\n",
    "# text_encoder.eval()\n",
    "\n",
    "# Optimize the model w/ IPEX\n",
    "pipe.unet = ipex.optimize(pipe.unet.eval(), dtype=infer_dtype, inplace=True)\n",
    "pipe.vae = ipex.optimize(pipe.vae.eval(), dtype=infer_dtype, inplace=True)\n",
    "pipe.text_encoder = ipex.optimize(pipe.text_encoder.eval(), dtype=infer_dtype, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e520b3e-2260-4182-bf1f-f13e72953696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-13 16:56:01,133] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-10-13 16:56:02,744] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-10-13 16:56:02,784] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n",
      "[2023-10-13 16:56:10,951] torch._inductor.compile_fx: [INFO] Step 3: torchinductor compiling FORWARDS graph 0\n",
      "[2023-10-13 16:56:11,032] torch._inductor.graph: [INFO] Creating implicit fallback for:\n",
      "  target: torch_ipex.ipex_linear.default\n",
      "  args[0]: TensorBox(StorageBox(\n",
      "    ComputedBuffer(name='buf3', layout=FlexibleLayout('cpu', torch.bfloat16, size=[1, 77, 1024], stride=[78848, 1024, 1]), data=Pointwise(\n",
      "      'cpu',\n",
      "      torch.bfloat16,\n",
      "      tmp0 = load(arg373_1, i1)\n",
      "      tmp1 = load(arg0_1, i2 + 1024 * (tmp0))\n",
      "      tmp2 = load(arg372_1, i1)\n",
      "      tmp3 = load(arg1_1, i2 + 1024 * (tmp2))\n",
      "      tmp4 = tmp1 + tmp3\n",
      "      tmp5 = to_dtype(tmp4, torch.float32)\n",
      "      tmp6 = load(buf1, i1)\n",
      "      tmp7 = tmp5 - tmp6\n",
      "      tmp8 = load(buf2, i1)\n",
      "      tmp9 = index_expr(1024, torch.float32)\n",
      "      tmp10 = tmp8 / tmp9\n",
      "      tmp11 = constant(1e-05, torch.float32)\n",
      "      tmp12 = tmp10 + tmp11\n",
      "      tmp13 = rsqrt(tmp12)\n",
      "      tmp14 = tmp7 * tmp13\n",
      "      tmp15 = load(arg2_1, i2)\n",
      "      tmp16 = tmp14 * tmp15\n",
      "      tmp17 = load(arg3_1, i2)\n",
      "      tmp18 = tmp16 + tmp17\n",
      "      tmp19 = to_dtype(tmp18, torch.bfloat16)\n",
      "      return tmp19\n",
      "      ,\n",
      "      ranges=[1, 77, 1024],\n",
      "      origins={mul, convert_element_type_2, var_mean, arg373_1, sub, arg2_1, view, convert_element_type_1, add_3, arg1_1, add, arg3_1, add_2, embedding, slice_1, mul_1, embedding_1, arg372_1, rsqrt, arg0_1}\n",
      "    ))\n",
      "  ))\n",
      "  args[1]: TensorBox(StorageBox(\n",
      "    InputBuffer(name='arg5_1', layout=FixedLayout('cpu', torch.bfloat16, size=[1024, 1024], stride=[1024, 1]))\n",
      "  ))\n",
      "  args[2]: TensorBox(StorageBox(\n",
      "    InputBuffer(name='arg4_1', layout=FixedLayout('cpu', torch.bfloat16, size=[1024], stride=[1]))\n",
      "  ))\n",
      "  args[3]: TensorBox(StorageBox(\n",
      "    Pointwise(\n",
      "      'cpu',\n",
      "      torch.int64,\n",
      "      tmp0 = index_expr(0, torch.int32)\n",
      "      tmp1 = index_expr(0, torch.int32)\n",
      "      tmp2 = tmp0 == tmp1\n",
      "      tmp3 = constant(94493912075648, torch.int64)\n",
      "      tmp4 = load(buf4, 0)\n",
      "      tmp5 = where(tmp2, tmp3, tmp4)\n",
      "      return tmp5\n",
      "      ,\n",
      "      ranges=[1],\n",
      "      origins={full_like, empty, select_scatter, select}\n",
      "    )\n",
      "  ))\n",
      "  args[4]: 1024\n",
      "[2023-10-13 16:56:11,039] torch._inductor.graph: [INFO] Using FallbackKernel: torch.ops.torch_ipex.ipex_linear.default\n",
      "[2023-10-13 16:56:30,562] torch._inductor.compile_fx: [INFO] Step 3: torchinductor done compiling FORWARDS graph 0\n",
      "[2023-10-13 16:56:30,569] torch._dynamo.output_graph: [INFO] Step 2: done compiler function debug_wrapper\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052b9ff29d9c4878ba43e744d98efe90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-13 16:56:30,821] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward\n",
      "[2023-10-13 16:56:33,915] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)\n",
      "[2023-10-13 16:56:33,956] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function debug_wrapper\n"
     ]
    }
   ],
   "source": [
    "generator = torch.Generator(\"cpu\").manual_seed(seed)\n",
    "\n",
    "# Simple timing of inference\n",
    "start = time.time()\n",
    "with torch.cpu.amp.autocast():\n",
    "    image = pipe(prompt, num_inference_steps=n_inf_steps, generator=generator).images[0]\n",
    "end = time.time()\n",
    "sd_bf16_t = end-start\n",
    "print(f\"Generating one BF16 image took {round(sd_bf16_t, 2)}s\")\n",
    "\n",
    "image.save(\"frog_test_BF16.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66231297-c716-46ea-8214-6f670513ed0b",
   "metadata": {},
   "source": [
    "We compare the inference speeds between the two runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41ce36-c71a-4bb0-9c0b-338957fe9fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inference with IPEX, using AMP+BF16, was {round(sd_fp32_t/sd_bf16_t, 2)}x faster.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694f007-b3f0-4e50-b1d0-e37cb65ceb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(outputdict):\n",
    "    fig = plt.figure(figsize = (10, 5)) \n",
    "    plt.bar(outputdict.keys(),outputdict.values(),color=['#ffd21e', '#0071c5'],width=0.4)\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.ylabel(\"Eval inference (seconds); lower is better\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47de3d4-d205-45ac-8856-5b7ba6aba757",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputDict={\"Full-precision\":sd_fp32_t,\"AMP (BF16)\":sd_bf16_t}\n",
    "plotter(outputDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipex_cpu",
   "language": "python",
   "name": "ipex_cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
